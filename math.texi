@subsection A single variable
EPL considers only variables with Gaussian distributions. That means the
probability density for a specific value @math{x} of a stochastic variable
@math{X} is given by 

@displaymath
   \rho_{X}(x)={1 \over \sqrt{2\pi\sigma^2}}
  \exp\left(-{1 \over 2} (x-\overline{X}) \sigma^{-2} 
                        (x-\overline{X}) \right)
@end displaymath

The two parameters @math{\overline{X}} and @math{\sigma} determines a Gaussian 
distribution completely. @math{\overline{X}} is the @math{x}-value with the
highest probability density and @math{\sigma} is known as error of @math{X}.

Since @math{\rho_{X}} is a density, the probability for a value
@math{x\in[x_i,x_j]} is obtained by an integral:

@displaymath
p_{X}(x_i<x<x_f)=\int_{x_i}^{x_f} dx' \; \rho_{X}(x')
@end displaymath

To complete our discussion we have to analyze a function @math{F} mapping a
stochastic variable into another. This function shall be defined by
a function @math{f} mapping all values of @math{X}, @math{x} to values of @math{Y}, @math{y}:

@displaymath
F(X)=Y \leftarrow f(x)=y
@end displaymath

the probability density of @math{Y} is given by@footnote{The
@math{\delta}-function is defined by 

@displaymath
g(y)=\int dx \; g(x) \delta(y-x)
@end displaymath
@displaymath
\widehat{\rho}_{Y}(y) =\int dx \; \rho_{X}(x) \delta(f(x)-y)
@end displaymath
}

Since we deal only with Gaussian errors in EPL we approximate @math{\widehat{p}_{Y}(w)}
by the Gaussian distribution. I add a superscript to @math{\sigma} in order to 
distinguish between the error of @math{X} and @math{Y}.

@math{ \overline{Y}=f(\overline{X})}
@displaymath
\sigma^{Y} = \left| {\partial f \over \partial x}(\overline{X}) \right|
                \sigma^{X}
@end displaymath

The approximation of the distribution @math{\widehat{p}} is given by the Gaussian
distribution with the parameters @math{\overline{Y}} and @math{\sigma_Y}. We
name this distribution @math{\rho_{Y}} instead of @math{\widehat{\rho}_{Y}}. Note that
@math{\rho_{Y} = \widehat{\rho}_{Y}} iff @math{f} is a linear function of @math{x}.

@subsection Multidimensional case

If several variables @math{X_i, i=1\ldots n} are independent,
the joint probability distribution is the product of the 
individual distributions.

@displaymath
  \rho_{\vec{X}}(\vec{x})=
  {1 \over \left(2\pi\right)^{n/2} \sqrt{\det(C)}} 
     \exp\left(-{1 \over 2} \sum_{i,j=1}^n 
      \left(x_i-\overline{X_i}\right) 
                \left(C_{ij}\right)^{-1} 
      \left(x_j-\overline{X_j}\right) \right)
@end displaymath

The @math{n\times n} Matrix @math{C} is symmetric and positive definite and it is called
correlation matrix. 
If the variables are uncorrelated @math{C} is a diagonal matrix composed of
@math{\sigma_i^2}.
For the inverse matrix of @math{C}, the curvature matrix the letter
@math{\alpha} is common.
If we have given @math{p_{\vec{X}}(\vec{x})} we can
obtain @math{\overline{X_i}} and @math{C_{ij}} via

@displaymath
\overline{X_i}=\int dx^n \; x_i p_{\vec{X}}(\vec{x})
@end displaymath
@displaymath
    C_{ij} = \int dx^n \;
      \left(x_i-\overline{X_i}\right)\left(x_j-\overline{X_j}\right)
    p_{\vec{X}}(\vec{x})
@end displaymath

Consider now a @math{m}-value function @math{f} of the @math{n} variables @math{\vec{x}}
defined by the values @math{\vec{f}(\vec{x})}. 
This function is used as in the one dimensional case to map the vector
@math{\vec{X}} of stochastic variables to the vector @math{\vec{Y}}.
As we will see, Stochastic requires @math{m\le n}.
The Gaussian approximation of the probability density is given by the
parameters:

@displaymath
\overline{Y_i}=f_i(\overline{X})
@end displaymath
@displaymath
C^{Y}_{kl} = \sum_{i,j=1}^{n} 
     {\partial f_k \over \partial x_i}(\overline{X})
        C^{X}_{ij}
     {\partial f_l \over \partial x_j}(\overline{X})
@end displaymath

Note that for @math{m>n} the correlation matrix @math{C^{Y}} is not positive
definite, but semi positive definite and that the approximation is
exact iff @math{f} is a linear function.

As a note unrelated to EPL: All the other distributions you learned in
your statistic class, like the @math{\chi^2} distribution, are exact
distributions of some functions (estimators) @math{f} over a set of
independent equally distributed stochastic variables (your sample).

In order to understand the correlation matrix we consider a two
dimensional example:
  
@displaymath
C=\left( C_{xx} \; C_{xy} \atop C_{yx} \; C_{yy} \right)
@end displaymath

In figure (3) you see in the @math{x}-@math{y} plane a curve of constant
probability density @math{k}. The maximum probability density is located in the 
middle of the curve @math{(\overline{X},\overline{Y})}, which is actually a 
ellipse. 

@image{ellipse}

The probability to find a @math{(x,y)} pair in the ellipse is equal to

@displaymath
p=\int_{0}^{k} dr \; \chi_d^2(r)
@end displaymath

where we have to set the number of dimensions to @math{d=2} for our
example. 
The relationship between the parameter of the ellipse and the correlation
matrix are given by

@displaymath
\sigma_{X}=\sqrt{k C_{xx}}
@end displaymath
@displaymath
\sigma_{Y}=\sqrt{k C_{yy}}
@end displaymath
@displaymath
s_{X}=\sqrt{k / \alpha_{xx}}
@end displaymath
@displaymath
s_{Y}=\sqrt{k / \alpha_{yy}}
@end displaymath

where @math{\alpha=C^{-1}}, the curvature matrix. Therefore the error of
a stochastic variable can be read of the correlation matrix. 

The eigenvalues of the matrix @math{C} are given by

@displaymath
\lambda_i={C_{xx}+C_{yy} \over 2 } \pm 
       \sqrt{{(C_{xx}-C_{yy})^2 \over 4} +C_{xy}^2 }
@end displaymath

and therefore the two half-axes by @math{a_i=\sqrt{k\lambda_i}}. The
ellipse is tilded by @math{\tan(\alpha)={\lambda_{+}-C_{xx}\over C_{xy}}},
and the eccentricity is given by @math{e=\sqrt{1-\lambda_{-}/\lambda_{+}}}

@section The EPL matrix

The EPL matrix is a superset of the correlation matrix in the sense
that a block of the EPL matrix is the correlation matrix. The EPL matrix
is symmetric and semi positive definite. To handle the formulas we 
switch to matrix notation, the formula (??) looks then like:

@displaymath
C^{Y} = F_{ki}(\overline{X}) C^{X} F^{t}(\overline{X})
@end displaymath
@displaymath
F_{ki}(\overline{X})= {\partial f_k \over \partial x_i}(\overline{X})
@end displaymath
 
The @math{k}-th row of @math{F} depends only on @math{f_k}, not on
@math{f_l}. We can extend the function @math{f} to @math{f'} where
@math{f'_i=f_i} for @math{1< i \le m} and @math{f'_i=x_i} for @math{m < i \le m+n}.
The EPL matrix @math{E^{f}} is calculated in the same way as @math{C^{Y}}:

@displaymath
E = F'_{ki}(\overline{X}) C^{X} F'{t}(\overline{X})
@end displaymath
@displaymath
F'_{ki}(\overline{X})= {\partial f'_k \over \partial x_i}(\overline{X})
@end displaymath 

and the upper left @math{m \times m } block of @math{E} is identical to @math{C^{Y}}. 
The matrix @math{F'} consist of an upper @math{m \times n } block identical to @math{F}
and a lower block identical to the @math{n \times n } identity matrix.

It is clear that a matrix @math{E} can be extended to include all dependent
and independent variables. In fact the matrix @math{E} does not know which
variables are included in the first place and which are generated by
applying a function on the variables. Furthermore the extension can be 
dynamically, we do not have to know how many variables @math{E} has to keep
at the end. At the same time we can delete a row and the corresponding
column associated with a  obsolete variable.
